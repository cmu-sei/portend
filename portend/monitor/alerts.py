#
# Portend Toolset
#
# Copyright 2024 Carnegie Mellon University.
#
# NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN "AS-IS" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.
#
# Licensed under a MIT (SEI)-style license, please see license.txt or contact permission@sei.cmu.edu for full terms.
#
# [DISTRIBUTION STATEMENT A] This material has been approved for public release and unlimited distribution.  Please see Copyright notice for non-US Government use and distribution.
#
# This Software includes and/or makes use of Third-Party Software each subject to its own license.
#
# DM24-1299
#

from __future__ import annotations

import math
import typing
from typing import Any, Dict, Optional

from portend.analysis.analyzer import calculate_metrics
from portend.analysis.predictions import Predictions
from portend.datasets.dataset import DataSet
from portend.metrics.basic import BasicMetric
from portend.utils.logging import print_and_log

ALERT_LEVEL_NONE = "none"
DEFAULT_NUM_ALERT_LEVELS = 4


metrics_cache: Dict[str, BasicMetric] = {}
"""Global metrics cache, used to persist state for metrics that require it."""


def clear_metrics_cache():
    """Clears up cache."""
    global metrics_cache
    metrics_cache.clear()


def calculate_metric_thresholds(
    metric_configs: list[dict[str, Any]],
    metric_stats: dict[str, dict[str, float]],
    num_alert_levels: Optional[int],
) -> dict[str, list[float]]:
    """Calculates the thresholds for a set of metrics."""
    metric_levels: dict[str, list[float]] = {}

    for metric in metric_configs:
        # print(metric)
        metric_name = metric["name"]
        levels = _calculate_default_thresholds(
            max=metric_stats[metric_name]["max"],
            min=metric_stats[metric_name]["min"],
            steps=num_alert_levels,
        )
        metric_levels[metric["name"]] = levels

    return metric_levels


def _calculate_default_thresholds(
    max: float, min: float, steps: Optional[int], abs_min: float = 0
) -> list[float]:
    """
    Compute a range of relative decreases scaled by a number of alert categories. Abs_min is used since it may not make sense
    to have additional alerting categories when the model performance is below some absolute value.

    :param max: the maximum value of the metric value.
    :param min: the minimum value of the metric value.
    :param steps: the max number of alert categories.
    :param abs_min: the absolute minimum value for metric value, to prevent alerting when model performance has degraded below this threshold.
    :returns: an array containing the default thresholds
    """
    if steps is None:
        steps = DEFAULT_NUM_ALERT_LEVELS

    thresholds: list[float] = []

    increment = math.ceil((max - min) / steps)
    curr_value = max
    for _ in range(steps):
        if curr_value >= abs_min:
            thresholds.append(curr_value)
            curr_value -= increment
        else:
            break

    # Round thresholds to avoid too many decimals.
    rounded_thresholds = [round(elem, 2) for elem in thresholds]

    # Return reversed list, since it was generated from largest threshold to smallest, but monitor needs smallest first.
    small_first_thresholds = list(reversed(rounded_thresholds))

    return small_first_thresholds


def calculate_alert_level(
    sample: Optional[dict[str, Any]],
    result: Any,
    additional_data: dict[str, Any],
    config: dict[str, Any],
) -> str:
    """
    Returns the alert level to execute depending on the current sample, model result, and configuration. Checks for each metric,
    and returns the alert level for the first threshold for the first metric that is found. Assume metrics are ordered in proper order to search them,
    and alerts are ordered from most critical to least.

    :param sample: A dictionary with all information needed about the sample to analyze the result of predicting based on it.
    :param result: Whatever the result was from the model.
    :param additional_data: A dictionary of name/values, where name is an id of additional data generated by the model, and values are those actual values.
    :param config: A dictionary of configuration, with thresholds and alert levels, from more critical to least. Example config:
        {"metrics": [
            {
                "name": "ATC",
                "metric_class": "portend.metrics.atc.atc.ATCMetric",
                "params":
                {
                "prep_module": "portend.examples.uav.wildnav_prep",
                "additional_data": "confidences"
                }
            }],
            "alerts": {
                "ATC":[
                    {"less_than": 0.85, "alert_level": "critical"}
                    {"less_than": 0.95, "alert_level": "warning"},
                ]}
        }
    :return: A string indicating the alert level, or the string "none" if no alert level is found.
    """
    # Translate any sample and result to actual datasets and predictions.
    if sample is None:
        datasets = None
    else:
        dataset = DataSet()
        dataset.set_samples(
            [
                dict(
                    {DataSet.DEFAULT_ID_KEY: "sample", "value": sample},
                    **sample,
                )
            ]
        )
        datasets = [dataset]
    prediction = Predictions()
    prediction.store_predictions([result])
    prediction.store_additional_data(additional_data)

    # Run metrics to get results.
    global metrics_cache
    metric_results = calculate_metrics(
        datasets, [prediction], config["metrics"], metric_cache=metrics_cache
    )

    # We go over the metrics, assuming they are ordered, and first one with a non "none" alert level will be used.
    alert_level = ALERT_LEVEL_NONE
    for metric in metric_results["metrics"]:
        metric_name = str(metric["name"])
        curr_metric_results = typing.cast(Dict[str, float], metric["results"])

        # We assume we should get only one result here, in the Monitor in operations.
        if len(curr_metric_results) != 1:
            raise Exception(
                f"Expected one result for metric {metric_name}, got {len(curr_metric_results)}"
            )
        metric_result = curr_metric_results[next(iter(curr_metric_results))]

        # Ignore metrics not configured with alerts.
        if metric_name not in config["alerts"]:
            print_and_log(
                f"Ignoring metric {metric_name} since was not found in list of configured alerts."
            )
            continue

        # We go over all alerts, assuming they are ordered, to find the first one that is true.
        print_and_log(
            f"Analyzing alert levels for metric {metric_name}, value {metric_result}"
        )
        alerts: list[dict[str, Any]] = config["alerts"][metric_name]
        for threshold in alerts:
            if metric_result < threshold["less_than"]:
                # If we matched a threshold, store that alert level and stop looping over following thresholds.
                alert_level = threshold["alert_level"]
                print_and_log(
                    f"Matched alert threshold: {threshold['less_than']}, selected alert level: {alert_level}"
                )
                break

        # If, for this metric, we found a non-none alert level, stop looking over metrics and return it.
        if alert_level != ALERT_LEVEL_NONE:
            break

    return alert_level
